{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rag1799/CLIP/blob/main/GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KtABEFeW-VRs"
      },
      "id": "KtABEFeW-VRs"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "2IgXlfSNlSdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c5e86a-932c-437f-db98-252d2ebe7638"
      },
      "id": "2IgXlfSNlSdi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/63.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.1 kB\u001b[0m \u001b[31m869.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m759.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we implement the models themselves. A simple Graph Convolutional Network (GCN) with 2 layers is already shown. Your first task will be to finish a Graph Attention Network (GAT) with variable number of layers, hidden_dim and heads as well as the option to define a dropout."
      ],
      "metadata": {
        "id": "PESohClSiPTD"
      },
      "id": "PESohClSiPTD"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# GNN classes need an __init__() and a forward() function\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        # Our GCN has two convolutional layers with a hidden dimension of 16 (16 \"neurons\")\n",
        "        self.conv1 = GCNConv(in_channels, 16)\n",
        "        self.conv2 = GCNConv(16, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        # For each layer perform the convolution and activation in between\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        # Return\n",
        "        return x\n",
        "\n",
        "# Now implement a Graph Attention Network with a variable number of layers, attention heads and hidden dimension - multiply the hidden dimension by the number of attention heads and set the attention heads to 1 for the output layer\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_dim = 8, num_layers = 2, dropout = 0.1, heads = 8):\n",
        "        super(GAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        # GATConv(in_channels, hidden_dim, heads=heads, dropout=dropout)  # Multi-head attention\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        # To prevent overfitting, you can artificially turn some neurons off, this is called \"dropout\"\n",
        "        # Perform a dropout (torch.dropout) after every activation\n",
        "        pass"
      ],
      "metadata": {
        "id": "9DJTjPTjiNWk"
      },
      "id": "9DJTjPTjiNWk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next comes the actual training of our networks. Typically, a data set is split into training set, validation set and test set. The training set is used for training and during training is validated using the validation set to reduce overfitting. Final performance is tested using the test set. Most things are already implemented here, you can add an early stopping mechanism if the model does not improve after a fixed amount of steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "PdUO3Y6aiyhJ"
      },
      "id": "PdUO3Y6aiyhJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d83e1f-ef38-427d-8cc2-31bf92d0ec67",
      "metadata": {
        "id": "42d83e1f-ef38-427d-8cc2-31bf92d0ec67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "3d183f04-713c-4e6a-ae96-5082b48efd94"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'with' statement on line 24 (ipython-input-3845810250.py, line 30)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3845810250.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    return loss.item(), acc\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'with' statement on line 24\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train(model, optimizer, data, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad() # make previous gradients 0\n",
        "    # Forward pass: model(data)\n",
        "    out = model(data)\n",
        "\n",
        "    # Calculate loss and perform backpropagation\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimization step\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = out[data.train_mask].argmax(dim=1) # filter by training data, get the class with the highest predicted value\n",
        "        acc = accuracy_score(data.y[data.train_mask].cpu(), pred.cpu()) # get the accuracy versus the true classes\n",
        "\n",
        "    return loss.item(), acc # loss is a tensor, convert to number\n",
        "\n",
        "def validate(model, data, criterion): # validate on validation set - this works similar to the training, just with .val_mask (and you dont need gradient tracking, so you can use torch.no_grad for everything)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # out =\n",
        "        # loss =\n",
        "        # pred =\n",
        "        # acc =\n",
        "\n",
        "    return loss.item(), acc\n",
        "\n",
        "def evaluate(model, data): # evaluate on test set - this time with .test_mask\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # out =\n",
        "        # pred =\n",
        "        # acc =\n",
        "\n",
        "    return acc\n",
        "\n",
        "def train_and_validate(model, optimizer, data, criterion, epochs, patience, delta):\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train(model, optimizer, data, criterion)\n",
        "        val_loss, val_acc = validate(model, data, criterion)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Implement the patience: If there is not at least a *delta* difference in *val_acc* in *patience* steps, stop training\n",
        "        # if val_acc > best_val_acc + delta:\n",
        "\n",
        "    test_acc = evaluate(model, data) # test set accuracy\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a plotting function to show the losses and accuracy over time"
      ],
      "metadata": {
        "id": "eaYpwnLeqcFN"
      },
      "id": "eaYpwnLeqcFN"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_training_plots(model_names, train_losses, train_accs, val_losses, val_accs, save_path):\n",
        "    fig, axes = plt.subplots(2, len(model_names), figsize=(18, 12))\n",
        "    colors = sns.color_palette('colorblind', len(model_names)) # works for at most 10 models\n",
        "    for i, (model_name, color) in enumerate(zip(model_names, colors)):\n",
        "      result = model_name[i]\n",
        "      if len(model_names) > 1:\n",
        "        axes[0, i].plot(train_losses[i], color=color, label='Train Loss')\n",
        "        axes[0, i].plot(val_losses[i], color=color, linestyle='--', label='Val Loss')\n",
        "        axes[0, i].set_title(f'{model_name} - Loss Curves')\n",
        "        axes[0, i].set_xlabel('Epoch')\n",
        "        axes[0, i].set_ylabel('Loss')\n",
        "        axes[0, i].legend()\n",
        "        axes[0, i].grid(True)\n",
        "        axes[1, i].plot(train_accs[i], color=color, label='Train Acc')\n",
        "        axes[1, i].plot(val_accs[i], color=color, linestyle='--', label='Val Acc')\n",
        "        axes[1, i].set_title(f'{model_name} - Accuracy Curves')\n",
        "        axes[1, i].set_xlabel('Epoch')\n",
        "        axes[1, i].set_ylabel('Accuracy')\n",
        "        axes[1, i].legend()\n",
        "        axes[1, i].grid(True)\n",
        "      else:\n",
        "        axes[0].plot(train_losses[i], color=color, label='Train Loss')\n",
        "        axes[0].plot(val_losses[i], color=color, linestyle='--', label='Val Loss')\n",
        "        axes[0].set_title(f'{model_name} - Loss Curves')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True)\n",
        "        axes[1].plot(train_accs[i], color=color, label='Train Acc')\n",
        "        axes[1].plot(val_accs[i], color=color, linestyle='--', label='Val Acc')\n",
        "        axes[1].set_title(f'{model_name} - Accuracy Curves')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Accuracy')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "g3aZUCqjmF0Z"
      },
      "id": "g3aZUCqjmF0Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we get into the data. The Cora citation network is stored in _Planetoid_ and we can get some information about the graph"
      ],
      "metadata": {
        "id": "BZdI_L1uqons"
      },
      "id": "BZdI_L1uqons"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (Cora citation network)\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "data = dataset[0]  # Get the graph data (Cora)\n",
        "\n",
        "print(dataset.num_classes) # shows the number of classes in the dataset (research areas)\n",
        "\n",
        "attributes = [attr for attr in dir(data)\n",
        "              if not callable(getattr(data, attr))\n",
        "              and not attr.startswith('_')]\n",
        "attributes.extend([attr for attr in data._store\n",
        "              if not callable(getattr(data, attr))\n",
        "              and not attr.startswith('__')])\n",
        "\n",
        "print(attributes)\n",
        "\n",
        "# These are attributes of our data set. Use this to get the size of the graph in terms of nodes and edges, as well as the size of the training, validation and test sets\n"
      ],
      "metadata": {
        "id": "pkatZU00uUoQ"
      },
      "id": "pkatZU00uUoQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we set up the model, train it on the Cora network data and plot the training process as well as get a final label prediction accuracy"
      ],
      "metadata": {
        "id": "-oMTlt2r-p3G"
      },
      "id": "-oMTlt2r-p3G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model instance - what are the in_channels and what are the out_channels? Try a couple of different parameters for your GAT to increase the accuracy\n",
        "gcn_model = GCN(dataset.num_node_features, dataset.num_classes)\n",
        "# gat_model =\n",
        "\n",
        "optimizer_gcn = optim.Adam(gcn_model.parameters(), lr=0.001) # Adam optimizer for minimizing the loss function, GAT can use the same\n",
        "# optimizer_gat =\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Loss function to minimize - when using CrossEntropyLoss, the forward function should return raw logits, when using other loss, the forward should return F.log_softmax(x) for multi-class classification\n",
        "\n",
        "gcn_train_loss, gcn_train_accs, gcn_val_loss, gcn_val_accs, gcn_accuracy = train_and_validate(gcn_model, optimizer_gcn, data, criterion, epochs=200, patience=10, delta=0.001)\n",
        "\n",
        "# update the plot to show both GCN and GAT\n",
        "create_training_plots([\"GCN\"], [gcn_train_loss], [gcn_train_accs], [gcn_val_loss], [gcn_val_accs], \"plot_GCN.png\")\n",
        "print(gcn_accuracy)"
      ],
      "metadata": {
        "id": "Ex2gldwOd41e"
      },
      "id": "Ex2gldwOd41e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}